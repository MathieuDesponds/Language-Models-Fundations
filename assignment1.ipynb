{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j05XaIOUvcf"
   },
   "source": [
    "#  Assignment 1 - Language model foundations 💬\n",
    "\n",
    "Welcome to the **1st assignment** for the **CS-552: Modern NLP course**!\n",
    "\n",
    "> - 😀 Name: **Mathieu Desponds**\n",
    "> - ✉️ Email: **mathieu.desponds@epfl.ch**\n",
    "> - 🪪 SCIPER: **283229**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0bVT0KPUvck"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;\">\n",
    "\n",
    "## How to implement this assignment\n",
    "\n",
    "Please read carefully the following points. All the information on how to read, implement and submit your assignment is explained in detail below.\n",
    "\n",
    "1. For this assignment, you will need to implement and fill in the missing code snippets for both the **Jupyter Notebook** `assignment1.ipynb` and the **`utils.py`** python file. In the `utils.py` file, you will add all the Dataset and Model classes you will implement according to the skeleton present in the file. In the notebook, you will add the data preprocessing pipeline for all the datasets, the training and testing pipelines of all implemented models and the report (See diagram below). \n",
    "    \n",
    "![assignment_1_arch.png](https://github.com/CS-552/a1-Mathiponds/blob/main/docs/assignment_1_arch.png?raw=1)\n",
    "\n",
    "2. To implement your coding part, you can import the external libraries we provide in the `requirements.txt` file, however, you should not use any other package non included in these requirements. \n",
    "\n",
    "3. At the end of the notebook, you will need to fill in a **report** template, providing the results of your implementation. We provide you with the template for the report, therefore you need to fill in the missing Markdown cells with the requested information. \n",
    "\n",
    "4. Along with the `assignment1.ipynb` and the `utils.py` files, you need to additionally upload models' pickle files under the `models/` dir, regarding the following models:\n",
    "    - the three LSTM-variant models (PART 2)  \n",
    "    - the trained-from-scratch Transformer model (PART 2) \n",
    "    - the fine-tuned Encoder-Decoder model (PART 3) \n",
    "    - the fine-tuned pre-trained Transformer model (PART 3)\n",
    "    \n",
    "You will provide test results on all of the model variants according to the report template.\n",
    "    \n",
    "5. Finally, you will need to log your training pipelines using Tensorboard. Please follow the instructions in the `README.md` of the [tensorboard/](tensorboard/README.md) directory.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA3bGKeGUvcl"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;\">\n",
    "\n",
    "## Assignment Description\n",
    "\n",
    "- In the first two parts of this assignment, you need to train and evaluate two different language models; an **LSTM-based model** and a **Transformer-based model**. You will first explore the distribution of the input data and perform data cleaning and pre-processing. Then you will build two language model training and testing pipelines implementing an LSTM and a Transformer language model. You will play around with different hyperparameters.\n",
    "- In the third part, you will build models on the downstream task of **Sentence Paraphrasing**. More specifically, you will fine-tune a sequence-2-sequence (**Encoder-Decoder**) architecture with attention and you will also fine-tune a **Transformer** model for this task.\n",
    "- Finally, you will fill out a report with the model results for the different parts. \n",
    "\n",
    "More specifically:\n",
    "\n",
    "- **[PART 1: Get to know your data](#1)**\n",
    "    - [1.1 Data Pre-processing](#11)\n",
    "    - [1.2 PyTorch Dataset creation](#12)\n",
    "- **[PART 2: Training Language Models](#2)**\n",
    "    - [2.1 LSTM-variants](#21)\n",
    "    - [2.2 Transformer-variants](#22)\n",
    "- **[PART 3: Fine-tune on the Text Paraphrasing task](#3)**\n",
    "    - [3.1 Train an Encoder-Decoder model on Text Paraphrasing](#31)\n",
    "    - [3.2 Run Transformer on Text Paraphrasing](#32)\n",
    "- **[PART 4: Write your report](#4)**\n",
    "    \n",
    "### Deliverables\n",
    "\n",
    "- ✅ This Jupyter notebook\n",
    "- ✅ `utils.py` file\n",
    "- ✅ 3 pickle files with the three LSTM-variant language models (Part 2)\n",
    "- ✅ Pickle file with the trained-from-scratch Transformer language model (Part 2)\n",
    "- ✅ Pickle file with the Encoder-Decoder model (Part 3)\n",
    "- ✅ Pickle file with the fine-tuned pre-trained Transformer model (Part 3)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBdny5anryRz"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: **Add your SCIPER number below as a `str`!**\n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8UEgVxw_sIrn"
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "SCIPER = \"283229\"\n",
    "\n",
    "try:\n",
    "    assert re.match(\"\\d{6}\", SCIPER)[0] == SCIPER, \"Invalid SCIPER given. please enter your correct 6-digit SCIPER number above!\"\n",
    "except:\n",
    "    print(\"Invalid SCIPER given. please enter your correct 6-digit SCIPER number above!\")\n",
    "\n",
    "student_seed = int(SCIPER[-4:])\n",
    "\n",
    "\n",
    "\"\"\"Set seed for reproducibility.\"\"\"\n",
    "random.seed(student_seed)\n",
    "np.random.seed(student_seed)\n",
    "torch.manual_seed(student_seed)\n",
    "torch.cuda.manual_seed_all(student_seed)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImPfdh9BmJem"
   },
   "source": [
    "### Packgage installation & importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eiPc1rRNiS__"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # limiting to one GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uexGOS0GiS__",
    "outputId": "78baccb9-4950-4219-930c-a50e7f7ca123",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\mathi\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache_beam in c:\\users\\mathi\\anaconda3\\lib\\site-packages (2.46.0)\n",
      "Requirement already satisfied: orjson<4.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (3.8.7)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (1.7)\n",
      "Requirement already satisfied: httplib2<0.22.0,>=0.8 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (0.21.0)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (3.13.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (4.4.0)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (1.51.3)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (1.7.3)\n",
      "Requirement already satisfied: protobuf<4,>3.12.2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (3.20.3)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (2.7.0)\n",
      "Requirement already satisfied: pytz>=2018.3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (2021.3)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (0.6.1)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (1.4.2)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (1.22.4)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (1.22.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (2.28.2)\n",
      "Requirement already satisfied: pyarrow<10.0.0,>=3.0.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (9.0.0)\n",
      "Requirement already satisfied: regex>=2020.6.8 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (2021.8.3)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (0.18)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (2.8.2)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from apache_beam) (2.2.1)\n",
      "Requirement already satisfied: docopt in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from httplib2<0.22.0,>=0.8->apache_beam) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2022.12.7)\n",
      "Installing collected packages: dill\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed dill-0.3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\mathi\\anaconda3\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torchmetrics) (1.22.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (1.9)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (4.4.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.6.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (3.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.1.2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.10.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from gensim==4.1.2) (1.22.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mathi\\anaconda3\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\mathi\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (4.62.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (2.10.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: dill in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (0.3.1.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (1.22.4)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.4)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mathi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: dill\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "Successfully installed dill-0.3.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\mathi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install apache_beam\n",
    "!pip install torchmetrics\n",
    "!pip install gensim==4.1.2\n",
    "!pip install transformers\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V9Fyf194mJeq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_writer = SummaryWriter(log_dir='tensorboard/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q0bmJE-0iTAA"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import torch\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoConfig, AutoModelWithLMHead, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m06WLo6qUvcm"
   },
   "source": [
    "---\n",
    "\n",
    "<a name=\"1\"></a>\n",
    "# PART 1: Get to know your data 🔎\n",
    "\n",
    "For the first two parts of this assignment, we will build our language models using the `wikitext-103` dataset.\n",
    "\n",
    "> The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified \n",
    "Good and Featured articles on Wikipedia. \n",
    "\n",
    "Bellow is an example from the dataset: \n",
    "<br>_(This example was too long and was cropped)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdwMZ-OUiTAB"
   },
   "source": [
    "\n",
    "\n",
    "<div style=\"padding:8px 0 8px 15px;background-color:#F3F3F3;border-radius:20px;\">\n",
    "<code>{\n",
    "\"text\": \"\\\" The Sinclair Scientific Programmable was introduced in 1975 , with the same case as the Sinclair Oxford . It was larger than t...\"\n",
    "}\n",
    "</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRxt_WZBiTAB"
   },
   "source": [
    "🧐 You can find more about this dataset [here](https://huggingface.co/datasets/wikitext)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOD9aSZIiTAC"
   },
   "source": [
    "<a name=\"11\"></a>\n",
    "## 1.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7A0sYxb8klR"
   },
   "source": [
    "In this part, while you get to better understand the dataset sturcuture, you will also do several steps to clean the dataset before passing them to neural models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADe8a4xOUvcm",
    "outputId": "420e3652-df89-4f84-a306-ea3918634175"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/mathi/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset is 1801350\n"
     ]
    }
   ],
   "source": [
    "# Loads the dataset\n",
    "wikitext_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "\n",
    "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfMJBW7OAnj-"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: **Filter out all empty sentences**.\n",
    "\n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXz9W7tZiTAD",
    "outputId": "f139c393-f106-49a3-c0ff-8547b9b03f15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-83ce11d964bf3208.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset is 1165029\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "wikitext_dataset = wikitext_dataset.filter(lambda example : len(example['text']) > 0)\n",
    "\n",
    "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdr5LIY5Dym3"
   },
   "source": [
    "Long sequences in the language model training can significantly slow down the training progress, both for RNN-based and transformer-based models.\n",
    "\n",
    "One of the tricks that is mentioned in [BERT's paper](https://arxiv.org/abs/1810.04805) is to perform pretraining with shorter sequences in the beginning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sIemLH6iTAF"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Following the same line of reasoning, **keep only samples that have at most 128 tokens**.\n",
    "    \n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9_xgDXJiTAG",
    "outputId": "564b0d30-5ae1-4b26-92ea-846ab09da99e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-69348c0197835f0c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset is 818092\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "wikitext_dataset = wikitext_dataset.filter(lambda example : len(example['text'].split(' ')) <= 128)\n",
    "\n",
    "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9LcAg0EJzrz"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Let's make the dataset samples **lower case** to decrease the vocabulary size.\n",
    "\n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8-Y-7JQiTAH",
    "outputId": "b7aab30f-eb53-47c7-c122-4c0a2958a10a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-578204af6221e85f.arrow\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def lower_text(example):\n",
    "    example['text'] = example['text'].lower()\n",
    "    return example\n",
    "wikitext_dataset = wikitext_dataset.map(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUjCH6Z-NALB"
   },
   "source": [
    "If you take a look at the first few samples of the dataset, you will notice that they belong to [this](https://en.wikipedia.org/wiki/Valkyria_Chronicles_II) Wikipedia article.\n",
    "\n",
    "We notice that **the title of  sections/articles are also included** in the dataset (for instance the title itself, or the `gameplay` section in this article). These samples are not very useful for language modeling, due to not having a sentence structure.\n",
    "Given the pattern of these samples, we need to **filter them out** from the dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTuUlhFuiTAI"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Filter out the samples with `= = <section> = = \\n` patterns.\n",
    "    \n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4snXCQBZiTAJ",
    "outputId": "1aef548a-d110-4358-e7bf-a500643c9cc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-31d000c7f8baa6af.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset is 513019\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "TITLE_TAG_RE = re.compile(r'(=\\s)+[^=]+(\\s=)+\\s\\n')\n",
    "def check_if_title(example):\n",
    "    is_title = TITLE_TAG_RE.search(example['text'])\n",
    "    return is_title == None\n",
    "wikitext_dataset = wikitext_dataset.filter(check_if_title)\n",
    "\n",
    "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16AHClg64IE6"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Normalize accented letters** (e.g., `clément` becomes `clement`) from text using `gensim.utils.deaccent` to further decrease vocabulary size.\n",
    "    \n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_IHLtBriTAK",
    "outputId": "a33a6d43-eae5-47e6-8d5d-578ac2dc3c7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-14acb047c5f17d6e.arrow\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from gensim.utils import deaccent\n",
    "def text_deaccent(example):\n",
    "    example['text'] = deaccent(example['text'])\n",
    "    return example\n",
    "wikitext_dataset = wikitext_dataset.map(text_deaccent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiD093zf5ie_"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Remove all samples having **non-english characters**. \n",
    "   \n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe along with the provided function `isEnglish()`. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xf0eznSxiTAL",
    "outputId": "5128989f-a44c-4204-9fb4-9391180bf681"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-97913c1a41697ac6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset is 425733\n"
     ]
    }
   ],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def isEnglish_seq(example):\n",
    "    tokens = example['text'].split(' ')\n",
    "    for t in tokens :\n",
    "        if not isEnglish(t):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "wikitext_dataset = wikitext_dataset.filter(isEnglish_seq)\n",
    "\n",
    "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dboQyQoD9jVx"
   },
   "source": [
    "### Looking at the vocabulary\n",
    "\n",
    "Before we move into additional preprocessing (similarly with the dataset used in exercises for week 2), we will take a look at the vocabulary size of the dataset until this point. We will assume that tokens can be simply splitted by `\" \"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TU6bUxgGiTAM"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Compute the frequency of all tokens in the dataset.**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1M0zpagYiTAM",
    "outputId": "c1139091-81b4-48aa-aa2b-1381a478933c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size of the dataset is 188017\n"
     ]
    }
   ],
   "source": [
    "def compute_token_frequency(dataset):\n",
    "  # YOUR CODE HERE\n",
    "    vocab_frequency = Counter()\n",
    "    for ex in dataset : \n",
    "        vocab_frequency.update(Counter(ex['text'].split(' ')))\n",
    "    return vocab_frequency\n",
    "\n",
    "vocab_frequency = compute_token_frequency(wikitext_dataset)\n",
    "print(f\"\\nVocabulary size of the dataset is {len(vocab_frequency)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1XjEaDvWJtB"
   },
   "source": [
    "As discussed in the lectures, real text datasets have a relatively high fraction of rare tokens. For that reason, let's visualize the histogram of token frequencies to better see this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dU97c1miTAN"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Plot a **histogram** with the frequencies of the words of the vocabulary. \n",
    "   \n",
    "💻 API hint: You can use the `matplotlib.hist` function. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "rBKUzDEDiTAN",
    "outputId": "f2bccec7-bccc-40b3-9c32-0ee0f6280306"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of words (log scale)')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "freq = vocab_frequency.values()\n",
    "plt.hist(np.log(list(freq)), log= True)\n",
    "plt.title('Histogram with the frequencies of the words of the vocabulary in loglog plot')\n",
    "plt.xlabel('Frequencies (log scale)')\n",
    "plt.ylabel('Number of words (log scale)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-vDgr_5HyrE"
   },
   "source": [
    "As you saw in the cells above, the dataset vocabulary is quite huge. Let's consider every token that occurs less than (or equal to) 5 times as a rare token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf5G9QFyiTAN"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Put these rare tokens in the **`rare_tokens` variable** and replace every rare token in the dataset with the `<unk>` token.\n",
    "    \n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52,
     "referenced_widgets": [
      "fe8747d657e54583adb6dd4eb48eaad2",
      "70c304c708574f77a272f08efec13b93",
      "1c62c3a9d80e4924b38a496df7b5c474",
      "9f2a6363b53f4ef18e6e3ab24ab2e844",
      "8fa8314b1f544b0dbe2a6b66642bb2ea",
      "49118f8559a4451eb0e71ba77175bd48",
      "575e52fadb8a4b6fb9e5d1753e805bfc",
      "ad2c50c10d624241a7189045c98395f6",
      "217bdb23b859459cac9dfc4593069614",
      "fc88e25d2a8b485aa32ecb60ba349e59",
      "3ab518d711e540588bcd56e726be8b03"
     ]
    },
    "id": "wRWJFfThiTAO",
    "outputId": "c8a65f4d-0d5d-4234-bf09-ed36d4a873c3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/425733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With threshold of 5, we have 105676 rare tokens.\n",
      " The vocabulary size is now 82341\n"
     ]
    }
   ],
   "source": [
    "rare_threshold = 5\n",
    "rare_tokens = set()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "rare_tokens = {k for k,v in dict(vocab_frequency).items() if v <= rare_threshold }\n",
    "\n",
    "def remove_rare_tokens(example):\n",
    "    example['text'] = ' '.join([t if t not in rare_tokens else '<unk>' for t in example['text'].split()])\n",
    "    return example\n",
    "\n",
    "wikitext_dataset = wikitext_dataset.map(remove_rare_tokens)\n",
    "\n",
    "print(f\"With threshold of {rare_threshold}, we have {len(rare_tokens)} rare tokens.\\n\",\n",
    "      f\"The vocabulary size is now {len(vocab_frequency) - len(rare_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fu2W9EMiWrP"
   },
   "source": [
    "The dataset still includes many short samples which are not very useful for the language modeling task. We will filter out very short samples from the dataset. _(Note: Assume tokens can be achieved by simple `\" \"` splitting.)_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruh0XqGKiTAO"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    " 🎯 Goal: Filter out every sample that has **less than (or equal to) 5 tokens**.\n",
    "    \n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "870c1b3d39d94431b959f20fa0eb24f4",
      "cf0168a918834141b79519f2b2602d8b",
      "fa3830594d944a0b960a77fa61bd5e83",
      "74c6a5b729a549999ad756b7170fa5db",
      "3cd963e4d83b4c03a1947b66ef7ec0ea",
      "056ec29974324207a825d2f378983269",
      "c2fb95ba6d2249108ceae7e3a6702027",
      "3f5fd6de59394b77b520511f5f82adb9",
      "607a074d72954f23b3cc342d8b7729e5",
      "ff33a5ca496b4222933ed7adf159ff50",
      "c86f1e18624b49d3b87156b972fcb078"
     ]
    },
    "id": "2QMmBOwjiTAO",
    "outputId": "73e83e11-002b-4f35-de73-92380ba7905d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/425733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset is 401401\n"
     ]
    }
   ],
   "source": [
    "short_seq_threshold = 5\n",
    "\n",
    "# YOUR CODE HERE\n",
    "wikitext_dataset = wikitext_dataset.filter(lambda example : len(example['text'].split(' ')) >= 5)\n",
    "\n",
    "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9wvH77WiTAP"
   },
   "source": [
    "After replacing rare tokens with <unk>, we could have sentences like `<unk> <unk> <unk> <unk>` which are again not very useful for language modeling. We will **filter out samples that more than 5% of its tokens are `<unk>`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoC2HrwKlalG"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Filter out samples that more than 5% of its tokens are `<unk>`**\n",
    "    \n",
    "💻 API hint: Use `datasets.Dataset` utilities to manipulate the dataframe.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "aa53ed3abd16489ca16a07898ca79a27",
      "453177ecbed04ec69ffe086d3e1abe45",
      "3f9a076e27784393b377bd017bcbea65",
      "6db0d2fb72824c1fb049fae00e37848f",
      "f2cce5a1c14d4859b0a8d8bc92532901",
      "98ea1352e7f04a4690efd847780499b1",
      "a0fcaced60344b04aa3b6df1095066a9",
      "a7418b577b4b45819361cbb8c8e8f34f",
      "de7d3989d00048deb875760e44f85cec",
      "29fd1792d73b4524aa4b656c142c6bde",
      "0563702ed250427fb98b57c9a349867b"
     ]
    },
    "id": "xdiXr9p0iTAP",
    "outputId": "91531053-c2d0-4ac4-bedc-41b32f4c8e9e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/401401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset is 401401\n"
     ]
    }
   ],
   "source": [
    "unknown_token_threshold = 0.05  # every sample that more than 5% of its tokens are <unk> should be removed\n",
    "\n",
    "def is_less_5_perc_unknown(example):\n",
    "    c = Counter(example['text'].split(' '))\n",
    "    total = sum(c.values())\n",
    "    n_unk = c['<unk']\n",
    "    return n_unk /total < unknown_token_threshold\n",
    "\n",
    "# YOUR CODE HERE\n",
    "wikitext_dataset = wikitext_dataset.filter(is_less_5_perc_unknown)\n",
    "print(f\"Size of the dataset is {len(wikitext_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGtIxEa07-8K"
   },
   "source": [
    "Let's recalculate the vocabulary for the resulting dataset, to see the vocabulary of the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3-WohMHlXIo",
    "outputId": "3d5fb2da-cfb2-4274-86a6-58369844b4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vocabulary size of the dataset is 82334\n"
     ]
    }
   ],
   "source": [
    "vocab_frequency = compute_token_frequency(wikitext_dataset)\n",
    "\n",
    "print(f\"\\nvocabulary size of the dataset is {len(vocab_frequency)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1jjqhpI4mJfP"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset_preprocessed.pickle', 'wb') as handle:\n",
    "    pickle.dump(wikitext_dataset, handle)\n",
    "with open('vocab_frequency.pickle', 'wb') as handle:\n",
    "    pickle.dump(vocab_frequency, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OkiTh5PrmJfQ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dataset_preprocessed.pickle', 'rb') as handle:\n",
    "    wikitext_dataset = pickle.load(handle)\n",
    "with open('vocab_frequency.pickle', 'rb') as handle:\n",
    "    vocab_frequency = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07Q5by1kiTAQ"
   },
   "source": [
    "---\n",
    "\n",
    "<a name=\"12\"></a>\n",
    "## 1.2 PyTorch Dataset  creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXgKUIGbSdAu"
   },
   "source": [
    "After the pre-processing of the dataset, we will now create a `torch.Dataset` class for the wiki-text dataset. \n",
    "We need to do so, in order to transform the dataset to the right format for the language modeling task. \n",
    "The following steps should be implemented:\n",
    "\n",
    "- Add `<start>` and `<stop>` tokens at the beginning and end of a sentence respectively.\n",
    "- Add padding tokens ( `<pad>` ) at the end of the sentences.\n",
    "- Create a fallback to <unk> token if an unseen word is encoded.\n",
    "- Define dictionaries that map tokens to their respective index in the embedding matrix and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtKKcjq4iTAR"
   },
   "source": [
    "### Create the RNN Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYcATZ2XiTAR"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Go to the `utils.py` file, and fill in the `RNNDataset` class with your implemenation.**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SmSdlO_BBbFO"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datasets\n",
    "from src.utils import RNNDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG5flxTZiTAR"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Instantiate** the implemented RNNDataset.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FvovsTi0iTAS"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "rnn_dataset = RNNDataset(wikitext_dataset, MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_woATTryiTAS"
   },
   "source": [
    "### Split data into train and test\n",
    "\n",
    "Once we have created the dataset ready for the model training pipeline, we will split it into train and test datasets. Then we will pass them to a `DataLoader` class, following the same method we saw in the exercises session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFevD8HUiTAS"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Split** the implemented RNNDataset into train and test subsets.\n",
    "    \n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YTS-9IfliTAT"
   },
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "dataset_length = len(wikitext_dataset)\n",
    "train_length = math.floor(dataset_length * TRAIN_RATIO)\n",
    "test_length = dataset_length - train_length\n",
    "\n",
    "rnn_train_dataset, rnn_test_dataset = torch.utils.data.random_split(rnn_dataset,\n",
    "                                                               [train_length, test_length],\n",
    "                                                               generator=torch.Generator().manual_seed(student_seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi1TfgHbiTAT"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "    \n",
    "🎯 Goal:  Create `DataLoader` objects using `batch_size = 8` for the train and test subsets.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GEab9WiQiTAU"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(rnn_train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(rnn_test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZpVeuyWiTAU"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;\">\n",
    "\n",
    "🎉 Excellent work! By this point, you will have made all the needed steps to make your data ready for training. \n",
    "\n",
    "#### Part 1 - Checklist\n",
    "Here are the core building blocks you created and that you will need for Part 2:\n",
    "   \n",
    "- [X] `rnn_dataset`: A Dataset obj with the data, the vocabulary, the pad index, the max sequence length, and maps of idx to word type and vice versa. \n",
    "- [X] `train_dataloader`: A DataLoader obj with your training data\n",
    "- [X] `test_dataloader`: A DataLoader obj with your testing data\n",
    "\n",
    "\n",
    "_Tip: Try to familiarize yourself with these objects and what functionalities and attributes they provide._\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPDNtdxFUvcn"
   },
   "source": [
    "---\n",
    "\n",
    "<a name=\"2\"></a>\n",
    "# PART 2:  Training Language Models 🤗\n",
    "\n",
    "#### Language Model: a probabilistic model of a sequence of tokens.\n",
    "\n",
    "🔵 **What?**\n",
    "\n",
    "Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions. They are used in natural language processing (NLP) applications, particularly ones that generate text as an output. Some of these applications include, machine translation and question-answering.\n",
    "\n",
    "🟡 **How?**\n",
    "\n",
    "There are several different probabilistic approaches to modeling language, which vary depending on the purpose of the language model. From a technical perspective, the various types differ by the amount of text data they analyze and the math they use to analyze it (architecture). Some LMs we've already seen and will learn about during lectures are n-gram / count-based models, Recurrent Neural Networks (RNNs), and Transformer models. \n",
    "\n",
    "🟣 **Why?**\n",
    "\n",
    "Language modeling is crucial in modern NLP applications. It is the reason that machines can understand qualitative information. Each language model type, in one way or another, turns qualitative information into quantitative information. This allows people to communicate with machines as they do with each other to a limited extent. It is used directly in a variety of industries including tech, finance, healthcare, transportation, legal, military and government. Additionally, it's likely most people reading this have interacted with a language model in some way at some point in the day, whether it be through Google search, an autocomplete text function or engaging with a voice assistant.\n",
    "\n",
    "ℹ️ Source: [Original article](https://www.techtarget.com/searchenterpriseai/definition/language-modeling#:~:text=Language%20models%20determine%20word%20probability,predict%20or%20produce%20new%20sentences.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_CK0Zl7iTAU"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid gray;background-color:#F3F3F3;border-radius: 15px;\">\n",
    "\n",
    "In this part, you will train your own language models using the dataset created in Part 1.\n",
    "\n",
    "More specifically, you need to implement **5 different model variants**, train and test them to compute their perplexity.\n",
    "    \n",
    "| Model | Variant | Description |\n",
    "|:---- |:----- | :----- |\n",
    "| | Token embeddings trained from scratch | An LSTM model with a trainable token Embedding layer <br>that will be initialized randomly and trained from scratch along with the LM. |\n",
    "| **LSTM** | Pre-trained token embeddings & frozen | An LSTM model with pre-trained GloVe embeddings as input <br>that will be frozen while the LM is training. |\n",
    "|  | Pre-trained token embeddings & trainable | An LSTM model with pre-trained GloVe embeddings as input <br>that will be further trained along with the LM. |\n",
    "||||\n",
    "| **Transformer** | Trained from scratch | A Transformer based model that follows the architecture of [DistilGPT2](https://huggingface.co/distilgpt2). |\n",
    "|  | Pre-trained DistilGPT2 | A pre-trained Transformer based model called [DistilGPT2](https://huggingface.co/distilgpt2) <br>and will be used only for testing (not training). |\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "po9S7IYGxb1G",
    "outputId": "d3e435b8-4c87-4aa2-ef4e-46a1262fc802"
   },
   "source": [
    "---\n",
    "<a name=\"21\"></a>\n",
    "## 2.1 LSTM-variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Smi1xBm3iTAV"
   },
   "source": [
    "### 2.1.1 Implementing all LSTM variants in one Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7AkS2_0iTAV"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Go to the `utils.py` file, and fill in the `VanillaLSTM` class with your implemenation.**\n",
    "    \n",
    "💻 Implementation hint: You will create one model class for all variants. Try to incorporate all the different cases into one Model class.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIdJ9c9-Uvco"
   },
   "outputs": [],
   "source": [
    "from src.utils import VanillaLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pfHmcVCiTAW"
   },
   "source": [
    "### 2.1.2 Building training and testing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3i0pjS2iTAW"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  Implement training and testing pipelines.\n",
    "  \n",
    "💻 Implementation hint: Check the pipelines we created in the exercises sessions.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyo5izV8iTAW"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, variant):\n",
    "    \"\"\"\n",
    "    Main training pipeline. Implement the following:\n",
    "    - pass inputs to the model\n",
    "    - compute loss\n",
    "    - perform backward pass and update weights\n",
    "\n",
    "    :param model: \n",
    "    :param train_loader:\n",
    "    :param optimizer:\n",
    "    :param criterion: \n",
    "    return: \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for epoch in range(1):\n",
    "        for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, targets = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            train_loss = loss\n",
    "            if variant == 'LSTM_A' :\n",
    "                tb_writer.add_scalar(\"LSTM_LM_variant_A/train_loss\", train_loss, i)\n",
    "            if variant == 'LSTM_B' :  \n",
    "                tb_writer.add_scalar(\"LSTM_LM_variant_B/train_loss\", train_loss, i)\n",
    "            if variant == 'LSTM_C' :\n",
    "                tb_writer.add_scalar(\"LSTM_LM_variant_C/train_loss\", train_loss, i)\n",
    "    \n",
    "    \n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3MaSi_TiTAX"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, variant):\n",
    "    \"\"\"\n",
    "    Main testing pipeline. Implement the following:\n",
    "    - pass inputs to the model\n",
    "    - compute loss\n",
    "    - compute perplexity\n",
    "\n",
    "    :param model: \n",
    "    :param test_loader:\n",
    "    :param criterion: \n",
    "    return: \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Testing loop\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(test_loader)):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data.to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs, targets = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # print statistics\n",
    "            test_losses.append(loss.item())\n",
    "            \n",
    "            if variant == 'LSTM_A' :\n",
    "                tb_writer.add_scalar(\"LSTM_LM_variant_A/test_loss\", loss.item(), i)\n",
    "            if variant == 'LSTM_B' :  \n",
    "                tb_writer.add_scalar(\"LSTM_LM_variant_B/test_loss\", loss.item(), i)\n",
    "            if variant == 'LSTM_C' :\n",
    "                tb_writer.add_scalar(\"LSTM_LM_variant_C/test_loss\", loss.item(), i)\n",
    "    \n",
    "    test_loss = np.array(test_losses).mean()\n",
    "    perplexity = np.exp(test_loss)\n",
    "    if variant == 'LSTM_A' :\n",
    "        tb_writer.add_scalar(\"LSTM_LM_variant_A/test_loss\", test_loss, 0)\n",
    "        tb_writer.add_scalar(\"LSTM_LM_variant_A/test_perplexity\", perplexity, 0)\n",
    "    if variant == 'LSTM_B' :  \n",
    "        tb_writer.add_scalar(\"LSTM_LM_variant_B/test_loss\", test_loss, 0)\n",
    "        tb_writer.add_scalar(\"LSTM_LM_variant_B/test_perplexity\", perplexity, 0)\n",
    "    if variant == 'LSTM_C' :\n",
    "        tb_writer.add_scalar(\"LSTM_LM_variant_C/test_loss\", test_loss, 0)\n",
    "        tb_writer.add_scalar(\"LSTM_LM_variant_C/test_perplexity\", perplexity, 0)\n",
    "    \n",
    "    print(f'Test loss: {test_loss:.3f}')\n",
    "    print(f'Test Perplexity: {perplexity:.3f}')\n",
    "    return test_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PyjPXYsnNI6"
   },
   "source": [
    "### 2.1.3 Train and test LSTM variants\n",
    "\n",
    "For **all the LSTM variants** you will perform the following steps:\n",
    "\n",
    "1. Set hypeparameters\n",
    "2. Load embeddings if needed\n",
    "3. Instantiate the model and set training configurations\n",
    "4. Run training pipeline (from 2.1.2)\n",
    "5. Save the model\n",
    "6. Run testing pipeline and compute perplexity (from 2.1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pro1_GX0iTAX"
   },
   "source": [
    "#### LSTM Variant A: Embeddings trained from scratch\n",
    "\n",
    "An LSTM model with a trainable Embedding layer that will be initialized randomly and trained from scratch along with the LM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYml9WJiiTAX"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Set hyperparameters according to the objective of the model.\n",
    "      \n",
    "💻 Implementation hint: You can play arround with different values for `dropout_rate`, `lr` and `num_layers`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "npU3rud1iTAY"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "vocab_size = len(list(rnn_dataset.word_to_index.keys()))\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "lr = 1e-2  # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqARabVmiTAY"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Instantiate the **model, optimizer and loss**.\n",
    "      \n",
    "💻 Implementation hint: Choose your training settings according to the task you need to do.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Md5wrZODiTAY",
    "outputId": "f597e540-1ce1-4ecc-be09-f8d3ba9363b7"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model = VanillaLSTM(vocab_size, \n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 num_layers, \n",
    "                 dropout_rate,\n",
    "                 embedding_weights=None,\n",
    "                 freeze_embeddings=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqbaWk-niTAZ"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **training and testing pipelines** on **10% data** (train and test) and compute perplexity.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiBcO9vJmJfi"
   },
   "outputs": [],
   "source": [
    "split_length_train = math.floor(0.1*len(rnn_train_dataset))\n",
    "small_rnn_train_dataset, _ = torch.utils.data.random_split(rnn_train_dataset, [split_length_train, len(rnn_train_dataset)-split_length_train])\n",
    "\n",
    "split_length_test = math.floor(0.1*len(rnn_test_dataset))\n",
    "small_rnn_test_dataset, _ = torch.utils.data.random_split(rnn_test_dataset, [split_length_test, len(rnn_test_dataset)-split_length_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPfVkbT5mJfj"
   },
   "outputs": [],
   "source": [
    "small_train_dataloader = DataLoader(small_rnn_train_dataset, batch_size=8, shuffle=True)\n",
    "small_test_dataloader = DataLoader(small_rnn_test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pktk0srSeH4J",
    "outputId": "af40e4b5-effb-4e32-9b6b-3fb0bb7079df",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, small_train_dataloader, optimizer, criterion, 'LSTM_A')\n",
    "torch.save(model.state_dict(), 'models/lstm_with_random_token_embedding.pt')\n",
    "test(model, small_test_dataloader, criterion, 'LSTM_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azom5bf9nU__"
   },
   "source": [
    "#### LSTM Variant B: Pre-trained embeddings & frozen\n",
    "\n",
    "An LSTM model with pre-trained GloVe embeddings as input that will be frozen while the LM is training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3xaQCLEiTAZ"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Download **GloVe embeddings**.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kd7HcRw0iTAa",
    "outputId": "d800e381-86a9-41d0-d787-0260e3cd57e8"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "# Download the \"glove-wiki-gigaword-100\" embeddings\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxjduZePiTAa"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Create an embedding layer with dimensions that match the input of `VanillaLSTM` model and initialize it with random weights.\n",
    "    \n",
    "💻 API hint: Use `torch.nn.Embedding` class.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SxSFs2NiTAa"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "initial_embedding_weight = nn.Embedding(vocab_size, embedding_dim).weight\n",
    "initial_embedding_weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V9MhJv_iTAa"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Add each GloVe embedding in the respective position in the Embedding layer created in previous step.\n",
    "\n",
    "💻 API hint: Use `.key_to_index` and `.word_to_index` functions.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpb9htFliTAa",
    "outputId": "9a38b79f-0691-496b-c356-ad9a4df08287"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "for w in tqdm(rnn_dataset.word_to_index.keys()):\n",
    "    if w in glove_vectors :\n",
    "        initial_embedding_weight[rnn_dataset.word_to_index[w]] = torch.tensor(glove_vectors[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fw2Vln9miTAb"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Instantiate the **model, optimizer and loss**.\n",
    "      \n",
    "💻 Implementation hint: Choose your training settings according to the task you need to do.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85tKcWG0iTAb",
    "outputId": "9a2a6cd9-9cb6-43d6-c8dc-cbd962bfacb5"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model = VanillaLSTM(vocab_size, \n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 num_layers, \n",
    "                 dropout_rate,\n",
    "                 embedding_weights=initial_embedding_weight,\n",
    "                 freeze_embeddings=True).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_Uezm3ZiTAb"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **training and testing pipelines** on **10% data** (train and test) and compute perplexity.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "om8GetFxiTAb",
    "outputId": "f3b79954-617b-4f23-df76-c8e956069772"
   },
   "outputs": [],
   "source": [
    "train(model, small_train_dataloader, optimizer, criterion, 'LSTM_B')\n",
    "torch.save(model.state_dict(), 'models/lstm_with_frozen_glove_token_embedding.pt')\n",
    "test(model, small_test_dataloader, criterion, 'LSTM_B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35jsDup2iTAb"
   },
   "source": [
    "#### LSTM Variant C: Pre-trained embeddings & trainable\t\n",
    "An LSTM model with pre-trained GloVe embeddings as input that will be further trained along with the LM.\n",
    "\n",
    "_Note: Use the same embedding layer you instantiated with GloVe embeddings in the previous step_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCr4iCXqmZPm",
    "outputId": "30f3bef5-b15c-401b-bd7b-c5ea933dd2ba"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Instantiate the **model, optimizer and loss**.\n",
    "      \n",
    "💻 Implementation hint: Choose your training settings according to the task you need to do.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiM8ADB0ssYv",
    "outputId": "89d14765-3f62-4cea-fa6c-bf043e674df8"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model = VanillaLSTM(vocab_size, \n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 num_layers, \n",
    "                 dropout_rate,\n",
    "                 embedding_weights=initial_embedding_weight,\n",
    "                 freeze_embeddings=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeEbRuZ4iTAc"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **training and testing pipelines** on **10% data** (train and test) and compute perplexity.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vgy6soessg1",
    "outputId": "23b1677a-1db4-4558-911e-ec197b139456"
   },
   "outputs": [],
   "source": [
    "train(model, small_train_dataloader, optimizer, criterion, 'LSTM_C')\n",
    "torch.save(model.state_dict(), 'models/lstm_with_nonfreezed_glove_token_embedding.pt')\n",
    "test(model, small_test_dataloader, criterion, 'LSTM_C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuBG_yHto6Qz"
   },
   "source": [
    "---\n",
    "\n",
    "<a name=\"22\"></a>\n",
    "## 2.2 Transformer-variants\n",
    "\n",
    "For all Transformer vairants we will use the architecture of **DistilGPT2** model. DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. See more details in the [HuggingFace model card](https://huggingface.co/distilgpt2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieaNrQIJzow-"
   },
   "source": [
    "### 2.2.1 Train DistilGPT-2 from scratch\n",
    "\n",
    "You will perform the following steps:\n",
    "\n",
    "1. Load the config of the DistilGPT-2 model using the Transformers library.\n",
    "2. Load Model class from the config and the respective tokenizer.\n",
    "3. Change input dataset to fit with the tokenization mechanism of DistilGPT-2.\n",
    "4. Split dataset into train and test.\n",
    "5. Create DataLoaders for train and test subsets.\n",
    "6. Train the model from stratch.\n",
    "7. Test the model and compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUWqgNzvzoj-"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dsz8Xg36iTAd"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Load model config, model class and tokenizer.\n",
    "      \n",
    "💻 Implementation hint: You should load the **model instance** and not the pre-trained model weights. You should load the pre-trained tokenizer though.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1c4f6c9f51a94b2098fd4abf846833aa",
      "dac1ae60d627406985222dd877ad20af",
      "52a6b6d926c34be59599099e5e4b771f",
      "e10e4b4aface424b87e8248928a36e32",
      "27ac80c8511a4409ac863fa690a9eb1f",
      "9899df0bad1e48f8afe0d1b61e07c711",
      "47224a4f20d94fd9a4b7ead5d4d2c8d0",
      "af7677f9e6594badaf4d4d7bbd9e906a",
      "3f459475ef9f45d6a4af03e830e41af2",
      "b11727491af741fba5b03ca0817f0061",
      "d26aa64f296946a98ca3576896085d21",
      "5349e477f4cb4a7a9b87ab03cd583eb8",
      "dab207b41b5a49078c819d880d567253",
      "2cf7fc1ee3f64bc0a4704481c405420b",
      "db4f34481f3d4432b1131c4454bde02b",
      "f9be83a48d9d4ba6a04492be4baa9b68",
      "77575870420841008befbc39cd6e8ca3",
      "21bf399e8fe14773bd8534079790d912",
      "8bf386f2c2e849a99222da93aa608f09",
      "3dd541afdaef4a4b90a625d1f75f0afc",
      "a0d6d3f3104a49b5a1280877b0acc6fc",
      "7dc0ca656f554e239a2fae118cccbff6",
      "1b05e82a6078475098952021b038334f",
      "ed05681b63164f6d8e5588f778b08c78",
      "4e342ffdce5c4d558b462633152c2d75",
      "f78d8af780e3404fadd7e658ec6bdc21",
      "37c3773071f84ee39e97b185be9708f5",
      "ea0f9cbaaa0d4bd9b410aecae4592f70",
      "055e8d3c5ed1480f8589046c99478264",
      "599e20d2050a4b5fbfd34886bed112ae",
      "929e1dc11deb4311af776dcc6912bb64",
      "a65deebb01dd49a3a9dd24088dff5c54",
      "11fab0bcd0f043a1a9bc9bbf1516c98e",
      "7f060da79b60464e9e77be8928d9f4fb",
      "f59ec6ee3aeb411598d6aa6ece60723b",
      "ef1083cf5d3f485aaba7cce62b1e74bd",
      "e4bb191b4b3644b18a15bf9b73d40025",
      "4945858564e54117b502c082a8ccc1f3",
      "0b9f7aa5ff474ca7824da8d4deb34879",
      "2f382619a1a444638ef8bffe7ab5b79c",
      "880dff82e1a74f1199667bf6b553f649",
      "78b4151d77db41218cf85c3a7dd3acf6",
      "3420b2ee5afc4266864da21329ee302c",
      "e99ef1590d79471aa075846e13e987df"
     ]
    },
    "id": "QLaG6iasiTAd",
    "outputId": "75526c39-f7ee-4c5d-e20e-15629273bc0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GPT2TokenizerFast\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model_config = AutoConfig.from_pretrained(model_name)\n",
    "gpt2_scratch_model = AutoModelForCausalLM.from_config(model_config)\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7BA6SG-iTAd"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Implement the following steps according to the in-line comments.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "5a7e6a665cc04371be0e989c5a0ab81a",
      "7e9b5c6a22634dab97f520459441e783",
      "e5e4ac7e29074d219da9230ee9e8383e",
      "54ead15fa4434755ab711700a72ae2ac",
      "d00c808399e94ce7bc7681b1924c1317",
      "c7f0434eab6c4f0b9a75f4cecfe75066",
      "2e7ad9cdaa5f4099837b7880b943a049",
      "4a8578d1d78e485aa6eab99cbf4cdc90",
      "20c6f750d02f475bbf9ca928bac783ed",
      "f75ecc1f5855495b8f0450ccb92ea3d3",
      "f96ef97ccb8d44d3b1b0249cd4d146c6",
      "54e5ed00801f4db9925277e2f13c7241",
      "0be215db5a2a46d1995e84fead153c47",
      "fbd757d570cb4829afd6d9de85ada202",
      "1feb95a88e3d427ba6f3ec9c4da75332",
      "d0a9edad721b4bb490cf145d73246752",
      "66e617bd73d243ee8f27f541f03e4a30",
      "f4d72d569d114d84a6e84fea58ab9ef3",
      "2b5f62cbc19e4204878137b4b6efaf9e",
      "a4a598d65d944868b621ba31ebb60774",
      "25e1f9da06784f1c9d16e59288a0c8af",
      "ca603589eb194ffcb70952f53870ee23",
      "8e5f6259bae04504842f6a02565516cd",
      "2fbeb9357db141a1a09f2a7bd18ab50d",
      "e6b099aa61334bbcb4582fff406301bf",
      "cc45d070cce348b09288c0ad73476db0",
      "859c5cb0fd084931bd4f28eb29106ec1",
      "1cb34ee3b5654eeaa33d315c171d7fe8",
      "be55bb34c88b4dc9bc67129266bb4814",
      "39513218f84c424482efec652188b836",
      "679e6431ac7f48829e9843309510f6ac",
      "6dcd380a3cf74c20b0bf327b3b2bd4d7",
      "c8b4d5e0b934412cbb1459593437c85b"
     ]
    },
    "id": "QIEmTqi7iTAe",
    "outputId": "d3cccae2-cf47-4a09-c0a7-bdf0617acc9e"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# add pad_token the same as the EOS token to not increase vocab size\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# tokenize wikitext_dataset with pre-trained DistilGPT2 tokenizer\n",
    "def tokenize(example):\n",
    "    example = gpt_tokenizer(example['text'], \n",
    "                            max_length = MAX_SEQ_LENGTH,\n",
    "                            padding = 'max_length')\n",
    "    return example\n",
    "encoded_dataset = wikitext_dataset.map(tokenize)\n",
    "\n",
    "# filter out sentences with length more than MAX_SEQ_LENGTH\n",
    "limited_encoded_dataset = encoded_dataset.filter(lambda example :\n",
    "                                    len(example['text']) <= MAX_SEQ_LENGTH )\n",
    "\n",
    "# create input_ids and attention_mask columns in the dataset\n",
    "\n",
    "limited_encoded_dataset = limited_encoded_dataset.remove_columns(\"text\")\n",
    "limited_encoded_dataset = limited_encoded_dataset.with_format(\"torch\")\n",
    "limited_encoded_dataset = limited_encoded_dataset.map(lambda example:\n",
    "                                                      {\"labels\": example[\"input_ids\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnTTvoyMiTAe"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  **Split** the `limited_encoded_dataset` into train and test subsets.\n",
    "    \n",
    "💻 API hint: Use `torch.utils.data.random_split` method with the given `TRAIN_RATIO`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbv9vnLyiTAe"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "TRAIN_RATIO = 0.9\n",
    "dataset_length = len(limited_encoded_dataset)\n",
    "train_length = int(TRAIN_RATIO * dataset_length)\n",
    "test_length = dataset_length - train_length\n",
    "\n",
    "transformer_train_dataset, transformer_test_dataset = torch.utils.data.random_split(limited_encoded_dataset, [train_length, test_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WydKF1tIiTAf"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Set hyperparameters according to the objective of the model.\n",
    "      \n",
    "💻 Implementation hint: You can play arround with different values for `learning_rate`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JV7Zq35AiTAf"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-wikitext103\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate= lr,\n",
    "    save_steps=10000,\n",
    "    weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLOGX_LQiTAg"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **training** using the `Trainer` class on **10% of data**.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uk2-RIzEmJf6"
   },
   "outputs": [],
   "source": [
    "split_length_train = math.floor(0.1*len(transformer_train_dataset))\n",
    "small_transformer_train_dataset, _ = torch.utils.data.random_split(transformer_train_dataset, \n",
    "                                            [split_length_train, len(transformer_train_dataset)-split_length_train])\n",
    "\n",
    "split_length_test = math.floor(0.1*len(transformer_test_dataset))\n",
    "small_transformer_test_dataset, _ = torch.utils.data.random_split(transformer_test_dataset, \n",
    "                                            [split_length_test, len(transformer_test_dataset)-split_length_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyBsdxwmo4E7"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=gpt_tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt2_scratch_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_transformer_train_dataset,\n",
    "    eval_dataset=small_transformer_test_dataset,\n",
    "    tokenizer=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "W9gVEQ6nUvcp",
    "outputId": "f670670c-d5de-4b0d-f216-d2f669781457",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "torch.save(trainer.model.state_dict(), 'models/distilgpt2-lm-from-scratch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZsEBG8RiTAg"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **testing** using the `Trainer` class and compute perplexity.\n",
    "\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "HiuwHEY6iTAg",
    "outputId": "dc059b10-4db7-4bfd-96b4-5283ac063efb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "perplexity_from_scratch = math.exp(eval_result[\"eval_loss\"])\n",
    "print(f\"The perplexity on the test dataset is {perplexity_from_scratch:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHzd5C0inhLP"
   },
   "source": [
    "### 2.2.2 Run Pre-trained GPT-2 model\n",
    "\n",
    "After training your trained-from-scratch Transformer model in the previous section, you will now use a pre-trained model to find its perplexity. Therefore, we will only perform testing of the pre-trained model on the test dataset. \n",
    "\n",
    "You will perform the following steps:\n",
    "\n",
    "1. Load pre-trained model and tokenizer\n",
    "2. Run testing and compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJpB8U-1iTAh"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "model_id = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4Ei6aFliTAh"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Load pre-trained model and tokenizer.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz53m57qiTAh"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "gpt2_pretrained_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer_pretrained_gpt = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_pretrained_gpt.pad_token = gpt_tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6TGvwLXiTAh"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Set hyperparameters to set up the Trainer.\n",
    "      \n",
    "💻 Implementation hint: We will use only the inference part on the trainer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHrdXr64iTAh"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"pretrained_{model_id}-wikitext103\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=lr,\n",
    "    save_steps=10000,\n",
    "    weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-hvHgXTiTAi"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **testing** using the `Trainer` class and compute perplexity.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8n7jLG2iTAi"
   },
   "outputs": [],
   "source": [
    "pretrained_trainer = Trainer(\n",
    "    model=gpt2_pretrained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_transformer_train_dataset,\n",
    "    eval_dataset=small_transformer_test_dataset,\n",
    "    tokenizer=tokenizer_pretrained_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "Rm_7YZY6iTAi",
    "outputId": "215f43d6-0594-4d4c-e4b1-e4b047ad6cfb"
   },
   "outputs": [],
   "source": [
    "eval_result = pretrained_trainer.evaluate()\n",
    "perplexity_pretrained_model = math.exp(eval_result[\"eval_loss\"])\n",
    "print(f\"The perplexity of pretrained {model_id} on the test dataset is {perplexity_pretrained_model:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FxdACC5iTAi"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;\">\n",
    "    \n",
    "🎉  Excellent work! By this point, you will have implemented all language model variants.\n",
    "\n",
    "#### Part 2 - Checklist\n",
    "Here are the core building blocks you created and that you will need for Part 3:\n",
    "   \n",
    "- [X] LSTM-variants checkpoints.\n",
    "- [X] LSTM-variants ppl scores.\n",
    "- [X] Transformer-variants ppl scores.\n",
    "\n",
    "_Note: Don't forget to include the tensorboard log to every model you trained, as discussed in the `README.md` of `tensorboard/` dir._\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBdlJJKhiTAi"
   },
   "source": [
    "---\n",
    "\n",
    "<a name=\"3\"></a>\n",
    "# PART 3: Fine-tune on the Text Paraphrasing task 🚀\n",
    "\n",
    "In this part, we will fine-tune and test the language models into the downstream task of text Paraphrasing. \n",
    "\n",
    "For this task, we will use the [MRPC dataset](https://paperswithcode.com/dataset/mrpc). Microsoft Research Paraphrase Corpus (MRPC) is a corpus consisting of 5,801 sentence pairs collected from newswire articles. Each pair is labeled if it is a paraphrase or not by human annotators. \n",
    "\n",
    " ![mrpc.png](https://github.com/CS-552/a1-Mathiponds/blob/main/docs/mrpc.png?raw=1)\n",
    " \n",
    "## Models\n",
    "For this dataset, we will select only the ones that correspond to text paraphrasing (label 1). With those, we will test the model's ability to take as input a sentence and produce as output the paraphrased one. \n",
    "\n",
    "### Encoder-Decoder architectures: \n",
    "To create a sequence2sequence model, we will create an Encoder-decoder model with an attention mechanism similar to the week 3 exercises.\n",
    "\n",
    "More specifically you need to implement the following:\n",
    "- Preprocess the dataset to match with the format of the model's input.\n",
    "- Build a Encoder-Decoder model that will be trained from stratch on the text paraphrasing task.\n",
    "- Train and test your architectures and compute the train/validation loss score. \n",
    "\n",
    "### Transformer-based architectures:\n",
    "\n",
    "You will also run experiments with the pre-trained Transformer-based model as we did in Part 2. \n",
    "You will be using again DistilGPT2. More specifically you need to implement the following:\n",
    "\n",
    "- Preprocess the dataset to match with the format of the model's input.\n",
    "- Run training (fine-tuning) of the model on train dataset.\n",
    "- Run inference on the test set and compute evaluation scores (see section below).\n",
    "\n",
    "\n",
    "#### Evaluation for the Transformer model\n",
    "\n",
    "You will evaluate your model using ROUGE scores. \n",
    " \n",
    "**ROUGE score** stands for Recall-Oriented Understudy for Gisting Evaluation. In its simplest form ROUGE score is the quotient of the matching words under the total count of words in reference sentence. Regarding the denominator ROUGE is a recall oriented metric. \n",
    "\n",
    "![rouge.png](https://github.com/CS-552/a1-Mathiponds/blob/main/docs/rouge.png?raw=1)\n",
    "\n",
    "**ROUGE-L score** is based on the length of the longest common subsequence (LCS). To counter the disadvantages of a pure recall metric as in ROUGE-N, Rouge-L calculates the weighted harmonic mean (or f-measure) combining the precision score and the recall score.\n",
    "\n",
    "![rouge_l.png](https://github.com/CS-552/a1-Mathiponds/blob/main/docs/rouge_l.png?raw=1)\n",
    "\n",
    "ℹ️ Source: [Original article](https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html#bleu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qanO0ZkaiTAj"
   },
   "source": [
    "### Load MPRC dataset and extract the paraphrased ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "ed68913d91a04c2483cb93096e74e85d",
      "1409216cb4a34c05bff300118052c3cf",
      "8b46a9cb2df1453aa6cfa60667b1753e",
      "15e30cae578648579b18af41a9f108aa",
      "aefbfaafb91d4e61901ea44c28059202",
      "1611ceb3d2db4aa087a40349e160a7f7",
      "69f943ea848b4128b359ff1f70b99eea",
      "106bd7e497f449a8b7c9c990589060ce",
      "000dcc79395146e398c198ca52093d04",
      "5b27e76efcc44918b2d564f7565d6937",
      "4d94b7daca7647479efe407502e6ba45"
     ]
    },
    "id": "j0jmgYZoiTAj",
    "outputId": "aae549e9-6fa5-4ec2-abcc-0b4b47c028c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mathi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Found cached dataset glue (C:/Users/mathi/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b5048bd977471a8ad2b119d55d52c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# load dataset\n",
    "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "MAX_SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1M57YPoiTAk"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Keep only the **paraphrased pair** of sentences.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVjicww_iTAk",
    "outputId": "5fff9d71-8231-4cc7-9a9d-104ec0ebb52a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-4f8ee6cc1c3c47b6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-d36223fb82170b59.arrow\n",
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-5ceb54e4b5ad2ef2.arrow\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "mrpc_dataset = mrpc_dataset.filter(lambda example : example['label'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFpqFEzMiTAk"
   },
   "source": [
    "<a name=\"31\"></a>\n",
    "## 3.1 Train Encoder-Decoder models on Text Paraphrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVaWIpbWiTAk"
   },
   "source": [
    "In this part, you will preprocess the dataset to make it suitable for the Encoder-Decoder model by adding `<start>` and `<stop>` tokens on each sentences and then padding to the maximum sequence length. From now on, we will refer to sentence 1 as context and sentence 2 as reference. Finally, you will compute the train/validation loss score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMwsM3auiTAk"
   },
   "source": [
    "### Data Preprocessing for encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx7Wnw_TiTAk"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Add  `<start>` and `<stop>` tokens and pad the input to `MAX_SEQ_LENGTH` length.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGoFig2j5TYi",
    "outputId": "3713d02d-88b6-4ba4-8768-ca309700605c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-f53b7690cd2fccc7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-14759cb84ad17502.arrow\n",
      "Loading cached processed dataset at C:\\Users\\mathi\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-b6294db442b71612.arrow\n"
     ]
    }
   ],
   "source": [
    "rnn_word_to_idx = rnn_dataset.word_to_index\n",
    "PAD_VALUE  = 0\n",
    "\n",
    "def tokenize_artisanal(example):\n",
    "    \n",
    "    def add_start_stop(example):\n",
    "        example['sentence1'] = '<start> ' + example['sentence1'] + ' <stop>'\n",
    "        example['sentence2'] = '<start> ' + example['sentence2'] + ' <stop>'\n",
    "        return example\n",
    "\n",
    "    # pad sequences\n",
    "    def pad_sentences(example):\n",
    "        tokens = example[\"sentence1\"].split(\" \")\n",
    "        example['sentence1'] = \" \".join(tokens + [\" <pad>\"] * (MAX_SEQ_LENGTH - len(tokens)))\n",
    "        tokens = example[\"sentence2\"].split(\" \")\n",
    "        example['sentence2'] = \" \".join(tokens + [\" <pad>\"] * (MAX_SEQ_LENGTH - len(tokens)))\n",
    "        return example\n",
    "    \n",
    "    def tokenize(feature):\n",
    "        return [rnn_word_to_idx.get(w, rnn_word_to_idx[\"<unk>\"]) for w in example[feature].split(\" \")[:MAX_SEQ_LENGTH]]\n",
    "    \n",
    "    example = pad_sentences(add_start_stop(example))\n",
    "    example[\"context_input_ids\"] = tokenize('sentence1')\n",
    "    example[\"reference_input_ids\"] = tokenize('sentence2')\n",
    "    return example\n",
    "\n",
    "\n",
    "mrpc_dataset = mrpc_dataset.map(tokenize_artisanal)\n",
    "mrpc_dataset = mrpc_dataset.remove_columns([\"sentence1\", \"sentence2\"])\n",
    "mrpc_dataset = mrpc_dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5Xj_5kHoiTAl"
   },
   "outputs": [],
   "source": [
    "mrpc_train, mrpc_validation, mrpc_test = mrpc_dataset[\"train\"], mrpc_dataset[\"validation\"], mrpc_dataset[\"test\"]\n",
    "\n",
    "mrpc_train_dataloader = DataLoader(mrpc_train, batch_size=8, shuffle=True)\n",
    "mrpc_validation_dataloader = DataLoader(mrpc_validation, batch_size=8, shuffle=False)\n",
    "mrpc_test_dataloader = DataLoader(mrpc_test, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xQLggBjiTAl"
   },
   "source": [
    "### Run model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO08RQdCiTAm"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal:  Implement training and testing pipelines.\n",
    "  \n",
    "💻 Implementation hint: Check the pipelines we created in the exercises sessions.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CCdCpEgQmJgU"
   },
   "outputs": [],
   "source": [
    "def seq2seq_train(model, train_loader, eval_loader, optimizer, criterion, num_epoch):\n",
    "    \n",
    "    best_eval_loss = 0.5 # used to do early stopping\n",
    "        \n",
    "    # Training loop\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0\n",
    "        running_loss = 0\n",
    "        for i, data in tqdm(enumerate(train_loader)):\n",
    "            src = data['context_input_ids'].float().to(device)\n",
    "            tgt = data['reference_input_ids'].to(device)\n",
    "#             src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            logits = model(src, tgt[:-1,:])\n",
    "            \n",
    "            tgt_out = tgt[1:,:]\n",
    "            loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            tb_writer.add_scalar(\"LSTM_seq2seq_attention/train_loss\",\n",
    "                                 loss.item(), epoch*len(train_loader)+i)\n",
    "            if i % 30 == 29 :\n",
    "                print(f\"EPOCH {epoch} : {i} / { len(train_loader)}, running_loss = {running_loss/i}\")\n",
    "        \n",
    "        print(f\"epoch loss : {running_loss/len(train_loader)}\")\n",
    "        epoch_loss += running_loss/len(train_loader)\n",
    "        if epoch_loss < best_eval_loss:\n",
    "            break\n",
    "\n",
    "    return epoch_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GF6j6u8PmJgV"
   },
   "outputs": [],
   "source": [
    "def seq2seq_eval(model, eval_loader, criterion):\n",
    "    # this function should be called in the train loop to monitor the performance in validation set while training.\n",
    "    \n",
    "    running_loss = 0\n",
    "    for i, data in (enumerate(eval_loader)):\n",
    "        src = data['context_input_ids'].to(device)\n",
    "        tgt = data['reference_input_ids'].to(device)\n",
    "        logits = model(src, tgt)   \n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "        running_loss += loss.item()\n",
    "        tb_writer.add_scalar(\"LSTM_seq2seq_attention/validation_loss\",\n",
    "                                 loss.item(), i)\n",
    "    test_loss = running_loss / len(eval_loader)\n",
    "    perplexity = np.exp(test_loss)\n",
    "    print(f'Test loss: {test_loss:.3f}')\n",
    "    print(f'Test Perplexity: {perplexity:.3f}')\n",
    "\n",
    "    return test_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3glI_f1iTAn"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Instantiate the **model, optimizer and loss**.\n",
    "      \n",
    "💻 Implementation hint: Choose your training settings according to the task you need to do.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQYokthFiTAl",
    "outputId": "35fbaaeb-1abf-4279-eb4b-1de7e0384da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import your end-dec model\n",
    "# from importlib import reload\n",
    "# import src.utils\n",
    "# reload(src.utils)\n",
    "from src.utils import EncoderDecoder\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PM69YsT9iTAn",
    "outputId": "8c7d140c-b068-4088-d8ad-1c05220f86a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 97,018,273 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 512\n",
    "lr = 0.001\n",
    "NUM_EPOCHS = 16\n",
    "\n",
    "seq2seq_with_attention_model = EncoderDecoder(hidden_size = HIDDEN_DIM, \n",
    "                                              input_vocab_size = vocab_size, \n",
    "                                              output_vocab_size = vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    seq2seq_with_attention_model.parameters(), lr=lr\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_VALUE)\n",
    "num_params = sum(p.numel() for p in seq2seq_with_attention_model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gh2RofH6iTAn"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **training and testing pipelines**.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONqeXDy3iTAn",
    "outputId": "84bc7d50-6337-4fc0-c475-fd36a0f31719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [01:13,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 29 / 310, running_loss = 3.9931544024368812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [02:25,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 59 / 310, running_loss = 3.4444479699862205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [03:47,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 89 / 310, running_loss = 3.250114965974615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [05:11,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 119 / 310, running_loss = 3.1611103931394946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [06:33,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 149 / 310, running_loss = 3.0948812161516024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [07:49,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 179 / 310, running_loss = 3.0584515526308027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [09:05,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 209 / 310, running_loss = 3.0331961344303697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:23,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 : 239 / 310, running_loss = 2.9998142519755344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:25,  2.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14404/95020015.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m seq2seq_train(seq2seq_with_attention_model, \n\u001b[0m\u001b[0;32m      2\u001b[0m               \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmrpc_train_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m               \u001b[0meval_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmrpc_validation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               criterion = criterion, num_epoch = NUM_EPOCHS)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14404/2487643193.py\u001b[0m in \u001b[0;36mseq2seq_train\u001b[1;34m(model, train_loader, eval_loader, optimizer, criterion, num_epoch)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtgt_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq2seq_train(seq2seq_with_attention_model, \n",
    "              train_loader = mrpc_train_dataloader, \n",
    "              eval_loader = mrpc_validation_dataloader, \n",
    "              optimizer = optimizer, \n",
    "              criterion = criterion, num_epoch = NUM_EPOCHS)\n",
    "# saving the model\n",
    "# torch.save(seq2seq_with_attention_model.state_dict(), \"models/rnn_seq2seq_with_attention.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxLtDFrodUyg"
   },
   "outputs": [],
   "source": [
    "torch.save(seq2seq_with_attention_model.state_dict(), \"models/rnn_seq2seq_with_attention.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rb8UxCv1fdEa",
    "outputId": "18381142-445c-460a-87a9-e5d13637e7dd"
   },
   "outputs": [],
   "source": [
    "seq2seq_eval(seq2seq_with_attention_model, \n",
    "              eval_loader = mrpc_validation_dataloader, \n",
    "              criterion = criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W06_v7iiTAo"
   },
   "source": [
    "<a name=\"32\"></a>\n",
    "## 3.2 Run Transformer on Text Paraphrasing\n",
    "\n",
    "In this part you will need to concatinate the paraphrased pair of sentences into one sequence to serve as input. Then we will use this input to pass it to the DistilGPT2 model for fine-tuning and testing.\n",
    "The input should be the following:\n",
    "```\n",
    "<sentence_1> <eos> <sentence_2> <eos>\n",
    "```\n",
    "where `<eos>` is the tokenizer's end-of-sequence token.\n",
    "\n",
    "From now on, we will refer to sentence 1 as `context` and sentence 2 as `reference`. \n",
    "\n",
    "Here, we use a decoder-only model (DistilGPT2) which gets the **context** as input and generates the **reference** sequence (token-by-token). Given this token-by-token generation, the nature of the model is very similar to a language model; the major difference is that in general causal language models try to predict the next token for the whole input, whereas in this case, the model should generate only the **reference**. (i.e., the **context** should be masked for loss computation).  \n",
    "\n",
    "Finally, you will compute the ROUGE scores as follows:\n",
    "\n",
    "1. You will generate 5 sequences given each context.\n",
    "2. You will compute the ROUGE-L score among these 5 generations and the **context** => `ROUGE(context, generationX)`\n",
    "3. You will select the best generation (among the 5 ones) as the predicted reference.\n",
    "4. You will compute the ROUGE-(1, 2, L) scores between the top generation (from step 3) and the **reference** => `ROUGE(reference, top-generation)`\n",
    "5. You will provide the average ROUGE-(1, 2, L) scores for all the test dataset samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp3K01PyiTAo"
   },
   "source": [
    "### Data Preprocessing for Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEn_yV-LiTAo"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Load pre-trained **model** and **tokenizer**.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltHbaf6riTAo"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"distilgpt2\"\n",
    "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "gpt2_pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer_pretrained_gpt = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMuY-DEciTAo"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Concatenate sentences, pass them to the tokenizer and clip to `MAX_SEQ_LENGTH` length.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2vZsSofiTAp"
   },
   "outputs": [],
   "source": [
    "tokenizer_pretrained_gpt.pad_token = tokenizer_pretrained_gpt.eos_token\n",
    "eos_token = tokenizer_pretrained_gpt.eos_token\n",
    "eos_token_id = tokenizer_pretrained_gpt.eos_token_id\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# concatenate sentences along with <eos> and pass them to the tokenizer\n",
    "mrpc_dataset = mrpc_dataset.map(lambda example : \n",
    "                               tokenizer_pretrained_gpt(example['sentence1'] + \n",
    "                        eos_token + example['sentence2'] + eos_token,\n",
    "                               max_length = MAX_SEQ_LENGTH,\n",
    "                               padding = 'max_length',\n",
    "                               truncation = True))\n",
    "\n",
    "# cut input and attention mask to MAX_SEQ_LENGTH\n",
    "def cut_input_attention_mask(example):\n",
    "    example['input_ids'] = example['input_ids'][0:MAX_SEQ_LENGTH]\n",
    "    example['attention_mask'] = example['attention_mask'][0:MAX_SEQ_LENGTH]\n",
    "    return example\n",
    "mrpc_dataset = mrpc_dataset.map(cut_input_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-tJGMKVmJgf"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Apply the masking technique described above (mask context sequence).\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_PMOpb0iTAp"
   },
   "outputs": [],
   "source": [
    "def get_sample_label(sample):\n",
    "    # this function masks the context (by assigning -100), and makes the paraphrase the target labels\n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    # ToDo really not sure\n",
    "    eos_pos = sample['input_ids'].index(eos_token_id)\n",
    "    output_label = [-100]*(eos_pos+1) + sample['input_ids'][eos_pos+1:]\n",
    "    return {\"labels\": output_label} \n",
    "\n",
    "mrpc_dataset = mrpc_dataset.map(get_sample_label)\n",
    "mrpc_dataset = mrpc_dataset.remove_columns(['sentence1', 'sentence2', 'label'])\n",
    "mrpc_dataset = mrpc_dataset.with_format(\"torch\")\n",
    "mrpc_train_dataset, mrpc_eval_dataset = mrpc_dataset[\"train\"], mrpc_dataset[\"validation\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WWoQKHXiTAp"
   },
   "source": [
    "### Run model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn2VyeWHiTAp"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Set hyperparameters according to the objective of the model.\n",
    "      \n",
    "💻 Implementation hint: You can play arround with different values for `learning_rate`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A46RPZ1xiTAq"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "\n",
    "# create the finetuning trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"finetune_{model_id}-MRPC\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=0.001,\n",
    "    num_train_epochs=20,\n",
    "    save_steps=10000,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\")\n",
    "\n",
    "gpt2_pretrained_model.transformer.wte.weight.requires_grad = False\n",
    "gpt2_pretrained_model.lm_head.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xlS2JYqiTAq"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: Run **training** using the `Trainer` class.\n",
    "      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Obg-hCQtiTAq",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "paraphrasing_trainer = Trainer(\n",
    "    model=gpt2_pretrained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=mrpc_train_dataset,\n",
    "    eval_dataset=mrpc_eval_dataset,\n",
    "    tokenizer=tokenizer_pretrained_gpt)\n",
    "\n",
    "paraphrasing_trainer.train()\n",
    "torch.save(paraphrasing_trainer.model.state_dict(), 'models/finetune-distilgpt2-mrpc-lr.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0honxSuiTAq"
   },
   "source": [
    "### Evaluate model with ROUGE scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-9rIf4tiTAq"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;\">\n",
    "\n",
    "🎯 Goal: For each sample in evaluation set, generate 5 outputs and perform the ROUGE evaluation as presented in the question description above.\n",
    "\n",
    "💻 Implementation hint: Use the following API call to get top-k generations\n",
    "    \n",
    "    generated_sequences = paraphrasing_trainer.model.generate(\n",
    "        context_ids,\n",
    "        do_sample=True, \n",
    "        max_length=MAX_SEQ_LENGTH, \n",
    "        top_k=20, \n",
    "        top_p=0.95, \n",
    "        no_repeat_ngram_size=2, \n",
    "        num_return_sequences=5\n",
    "    )\n",
    "\n",
    "_Note 1: For simplicity, you can ignore the contexts that have more than 1 sentence._\n",
    "\n",
    "_Note 2: On the generated reference, if there is more that 1 sentence generated, keep only the first one._\n",
    "\n",
    "_Note 3: To split into sentences, you can use [`nltk.sent_tokenize()`](https://www.nltk.org/api/nltk.tokenize.html)._\n",
    "\n",
    "_Note 4: Use the [`evaluate.load('rouge')`](https://huggingface.co/spaces/evaluate-metric/rouge) function to compute the ROUGE metrics._\n",
    "\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_pretrained_model.load_state_dict(torch.load('models/finetune-distilgpt2-mrpc.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "from rouge_score import rouge_scorer, scoring\n",
    "k = 5\n",
    "def generate_top_k_sample(sample, k =5):\n",
    "    context = sample['sentence1']\n",
    "    context_ids = tokenizer_pretrained_gpt(context)['input_ids']\n",
    "    generated_sentence_ids = gpt2_pretrained_model.generate(\n",
    "        torch.tensor(context_ids)[None, :],\n",
    "        do_sample=True, \n",
    "        max_length=MAX_SEQ_LENGTH, \n",
    "        top_k=20, \n",
    "        top_p=0.95, \n",
    "        no_repeat_ngram_size=2, \n",
    "        num_return_sequences=k\n",
    "    )\n",
    "    \n",
    "    generated_sentences = []\n",
    "    for generated_ids in generated_sentence_ids :\n",
    "        generated_sentences.append(tokenizer_pretrained_gpt.decode(generated_ids))\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rR4JM4_mJgm"
   },
   "outputs": [],
   "source": [
    "r_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "\n",
    "rouge_values = []\n",
    "for sample in tqdm(mrpc_eval_dataset):\n",
    "    generated_seqs = generate_top_k_sample(sample, k)\n",
    "\n",
    "    ## compute the rouge score of all the genreated sentences\n",
    "    rouges_1 = np.zeros(k)\n",
    "    rouges_2 = np.zeros(k)\n",
    "    rouges_l = np.zeros(k)\n",
    "    for i, generated_sequence in enumerate(generated_seqs):\n",
    "        rouge = r_scorer.score(generated_sequence, sample[\"sentence2\"])\n",
    "        rouges_1[i] = rouge['rouge1'].fmeasure\n",
    "        rouges_2[i] = rouge['rouge2'].fmeasure\n",
    "        rouges_l[i] = rouge['rougeL'].fmeasure\n",
    "        \n",
    "        \n",
    "        rouges[i] = r_scorer.score(generated_sequence, sample[\"sentence2\"])['rougeL'].fmeasure\n",
    "        \n",
    "    rouge_values.append((rouges_1.max(), rouges_2.max(),rouges_l.max()))\n",
    "\n",
    "## mean rouge score\n",
    "#print(\"> the mean rouge values is \", sum(rouge_values) / len(rouge_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1 = np.mean([i[0] for i in rouge_values])\n",
    "rouge_2 = np.mean([i[1] for i in rouge_values])\n",
    "rouge_l = np.mean([i[2] for i in rouge_values])\n",
    "rouge_l_sum = sum([i[2] for i in rouge_values])\n",
    "rouge_1, rouge_2, rouge_l, rouge_l_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbWpKRKpiTAr"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;\">\n",
    "\n",
    "🎉 Excellent work! You just finished the code implementation parts of the assignment. \n",
    "\n",
    "#### Part 3 - Checklist\n",
    "Here are the elements you will need for the report in Part 4:\n",
    "   \n",
    "- [X] LSTM-variants scores on perplexity and their checkpoints.\n",
    "- [X] DistilGPT2 score on perplexity and its checkpoint.\n",
    "- [X] Encoder-decoder variant train/validation loss score and its checkpoint.\n",
    "- [X] DistilGPT2 ROUGE scores and its fine-tuned checkpoint.\n",
    "\n",
    "_Note: Don't forget to include the tensorboard log to every model you trained._\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drEstqDkUvcp"
   },
   "source": [
    "---\n",
    "\n",
    "<a name=\"4\"></a>\n",
    "# PART 4: Write your report 📘\n",
    "\n",
    "Fill in the tables with the respective scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaCft7HUw4uh"
   },
   "source": [
    "<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;text-align:center;\">\n",
    "\n",
    "#### Perplexity results on Language Models\n",
    "\n",
    "| Model - Variant | PPL |\n",
    "|:--------- | :-----: |\n",
    "| LSTM Variant A - Embeddings trained from scratch | 26.694 |\n",
    "| LSTM Variant B - Pre-trained embeddings & frozen | 25.712 |\n",
    "| LSTM Variant C - Pre-trained embeddings & trainable | 23.966 |\n",
    "||||\n",
    "| DistilGPT2 - Trained from scratch | 2.324 |\n",
    "| Pre-trained DistilGPT2 | 2.275 |\n",
    "    \n",
    "#### Performance scores on Text Paraphrasing\n",
    "| Model - Variant | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-Lsum |\n",
    "|:--------- | :-----: | :-----: |  :-----: |  :-----: | \n",
    "| Pre-trained DistilGPT2 | 0.552892517748759 | 0.3862343325892713 | 0.49937688378066714 | 203.74576858251234 |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Outd6rdTmJgp",
    "outputId": "f28c3544-d254-4021-ac10-c1a54295e356"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "acc = EventAccumulator(\n",
    "    'tensorboard/events.out.tfevents.1679409484.6ae598a9be28.823.0'\n",
    ")\n",
    "acc.Reload()\n",
    "acc.Tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxqnN3GDmJgr",
    "outputId": "e4e4b551-dfab-4e2e-bd27-3df96e730d73"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(acc.Scalars('LSTM_LM_variant_B/train_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWeDkDxtmJgr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "000dcc79395146e398c198ca52093d04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "055e8d3c5ed1480f8589046c99478264": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0563702ed250427fb98b57c9a349867b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "056ec29974324207a825d2f378983269": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b9f7aa5ff474ca7824da8d4deb34879": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0be215db5a2a46d1995e84fead153c47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66e617bd73d243ee8f27f541f03e4a30",
      "placeholder": "​",
      "style": "IPY_MODEL_f4d72d569d114d84a6e84fea58ab9ef3",
      "value": "Filter: 100%"
     }
    },
    "106bd7e497f449a8b7c9c990589060ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11fab0bcd0f043a1a9bc9bbf1516c98e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1409216cb4a34c05bff300118052c3cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1611ceb3d2db4aa087a40349e160a7f7",
      "placeholder": "​",
      "style": "IPY_MODEL_69f943ea848b4128b359ff1f70b99eea",
      "value": "100%"
     }
    },
    "15e30cae578648579b18af41a9f108aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b27e76efcc44918b2d564f7565d6937",
      "placeholder": "​",
      "style": "IPY_MODEL_4d94b7daca7647479efe407502e6ba45",
      "value": " 3/3 [00:00&lt;00:00, 57.91it/s]"
     }
    },
    "1611ceb3d2db4aa087a40349e160a7f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b05e82a6078475098952021b038334f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ed05681b63164f6d8e5588f778b08c78",
       "IPY_MODEL_4e342ffdce5c4d558b462633152c2d75",
       "IPY_MODEL_f78d8af780e3404fadd7e658ec6bdc21"
      ],
      "layout": "IPY_MODEL_37c3773071f84ee39e97b185be9708f5"
     }
    },
    "1c4f6c9f51a94b2098fd4abf846833aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dac1ae60d627406985222dd877ad20af",
       "IPY_MODEL_52a6b6d926c34be59599099e5e4b771f",
       "IPY_MODEL_e10e4b4aface424b87e8248928a36e32"
      ],
      "layout": "IPY_MODEL_27ac80c8511a4409ac863fa690a9eb1f"
     }
    },
    "1c62c3a9d80e4924b38a496df7b5c474": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad2c50c10d624241a7189045c98395f6",
      "max": 425733,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_217bdb23b859459cac9dfc4593069614",
      "value": 425733
     }
    },
    "1cb34ee3b5654eeaa33d315c171d7fe8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1feb95a88e3d427ba6f3ec9c4da75332": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25e1f9da06784f1c9d16e59288a0c8af",
      "placeholder": "​",
      "style": "IPY_MODEL_ca603589eb194ffcb70952f53870ee23",
      "value": " 401000/401401 [00:46&lt;00:00, 9514.86 examples/s]"
     }
    },
    "20c6f750d02f475bbf9ca928bac783ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "217bdb23b859459cac9dfc4593069614": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "21bf399e8fe14773bd8534079790d912": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25e1f9da06784f1c9d16e59288a0c8af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27ac80c8511a4409ac863fa690a9eb1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29fd1792d73b4524aa4b656c142c6bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b5f62cbc19e4204878137b4b6efaf9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cf7fc1ee3f64bc0a4704481c405420b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bf386f2c2e849a99222da93aa608f09",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dd541afdaef4a4b90a625d1f75f0afc",
      "value": 1042301
     }
    },
    "2e7ad9cdaa5f4099837b7880b943a049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f382619a1a444638ef8bffe7ab5b79c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fbeb9357db141a1a09f2a7bd18ab50d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cb34ee3b5654eeaa33d315c171d7fe8",
      "placeholder": "​",
      "style": "IPY_MODEL_be55bb34c88b4dc9bc67129266bb4814",
      "value": "Map: 100%"
     }
    },
    "3420b2ee5afc4266864da21329ee302c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37c3773071f84ee39e97b185be9708f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39513218f84c424482efec652188b836": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ab518d711e540588bcd56e726be8b03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cd963e4d83b4c03a1947b66ef7ec0ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "3dd541afdaef4a4b90a625d1f75f0afc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3f459475ef9f45d6a4af03e830e41af2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3f5fd6de59394b77b520511f5f82adb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9a076e27784393b377bd017bcbea65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7418b577b4b45819361cbb8c8e8f34f",
      "max": 401401,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de7d3989d00048deb875760e44f85cec",
      "value": 401401
     }
    },
    "453177ecbed04ec69ffe086d3e1abe45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98ea1352e7f04a4690efd847780499b1",
      "placeholder": "​",
      "style": "IPY_MODEL_a0fcaced60344b04aa3b6df1095066a9",
      "value": "Filter: 100%"
     }
    },
    "47224a4f20d94fd9a4b7ead5d4d2c8d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49118f8559a4451eb0e71ba77175bd48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4945858564e54117b502c082a8ccc1f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a8578d1d78e485aa6eab99cbf4cdc90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d94b7daca7647479efe407502e6ba45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e342ffdce5c4d558b462633152c2d75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_599e20d2050a4b5fbfd34886bed112ae",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_929e1dc11deb4311af776dcc6912bb64",
      "value": 456318
     }
    },
    "52a6b6d926c34be59599099e5e4b771f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af7677f9e6594badaf4d4d7bbd9e906a",
      "max": 762,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f459475ef9f45d6a4af03e830e41af2",
      "value": 762
     }
    },
    "5349e477f4cb4a7a9b87ab03cd583eb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dab207b41b5a49078c819d880d567253",
       "IPY_MODEL_2cf7fc1ee3f64bc0a4704481c405420b",
       "IPY_MODEL_db4f34481f3d4432b1131c4454bde02b"
      ],
      "layout": "IPY_MODEL_f9be83a48d9d4ba6a04492be4baa9b68"
     }
    },
    "54e5ed00801f4db9925277e2f13c7241": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0be215db5a2a46d1995e84fead153c47",
       "IPY_MODEL_fbd757d570cb4829afd6d9de85ada202",
       "IPY_MODEL_1feb95a88e3d427ba6f3ec9c4da75332"
      ],
      "layout": "IPY_MODEL_d0a9edad721b4bb490cf145d73246752"
     }
    },
    "54ead15fa4434755ab711700a72ae2ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f75ecc1f5855495b8f0450ccb92ea3d3",
      "placeholder": "​",
      "style": "IPY_MODEL_f96ef97ccb8d44d3b1b0249cd4d146c6",
      "value": " 401401/401401 [03:40&lt;00:00, 1216.10 examples/s]"
     }
    },
    "575e52fadb8a4b6fb9e5d1753e805bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "599e20d2050a4b5fbfd34886bed112ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a7e6a665cc04371be0e989c5a0ab81a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e9b5c6a22634dab97f520459441e783",
       "IPY_MODEL_e5e4ac7e29074d219da9230ee9e8383e",
       "IPY_MODEL_54ead15fa4434755ab711700a72ae2ac"
      ],
      "layout": "IPY_MODEL_d00c808399e94ce7bc7681b1924c1317"
     }
    },
    "5b27e76efcc44918b2d564f7565d6937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "607a074d72954f23b3cc342d8b7729e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66e617bd73d243ee8f27f541f03e4a30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "679e6431ac7f48829e9843309510f6ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "69f943ea848b4128b359ff1f70b99eea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6db0d2fb72824c1fb049fae00e37848f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29fd1792d73b4524aa4b656c142c6bde",
      "placeholder": "​",
      "style": "IPY_MODEL_0563702ed250427fb98b57c9a349867b",
      "value": " 401401/401401 [00:10&lt;00:00, 49672.73 examples/s]"
     }
    },
    "6dcd380a3cf74c20b0bf327b3b2bd4d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70c304c708574f77a272f08efec13b93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49118f8559a4451eb0e71ba77175bd48",
      "placeholder": "​",
      "style": "IPY_MODEL_575e52fadb8a4b6fb9e5d1753e805bfc",
      "value": "Map: 100%"
     }
    },
    "74c6a5b729a549999ad756b7170fa5db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff33a5ca496b4222933ed7adf159ff50",
      "placeholder": "​",
      "style": "IPY_MODEL_c86f1e18624b49d3b87156b972fcb078",
      "value": " 425733/425733 [00:03&lt;00:00, 109176.88 examples/s]"
     }
    },
    "77575870420841008befbc39cd6e8ca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78b4151d77db41218cf85c3a7dd3acf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7dc0ca656f554e239a2fae118cccbff6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e9b5c6a22634dab97f520459441e783": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7f0434eab6c4f0b9a75f4cecfe75066",
      "placeholder": "​",
      "style": "IPY_MODEL_2e7ad9cdaa5f4099837b7880b943a049",
      "value": "Map: 100%"
     }
    },
    "7f060da79b60464e9e77be8928d9f4fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f59ec6ee3aeb411598d6aa6ece60723b",
       "IPY_MODEL_ef1083cf5d3f485aaba7cce62b1e74bd",
       "IPY_MODEL_e4bb191b4b3644b18a15bf9b73d40025"
      ],
      "layout": "IPY_MODEL_4945858564e54117b502c082a8ccc1f3"
     }
    },
    "859c5cb0fd084931bd4f28eb29106ec1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "870c1b3d39d94431b959f20fa0eb24f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf0168a918834141b79519f2b2602d8b",
       "IPY_MODEL_fa3830594d944a0b960a77fa61bd5e83",
       "IPY_MODEL_74c6a5b729a549999ad756b7170fa5db"
      ],
      "layout": "IPY_MODEL_3cd963e4d83b4c03a1947b66ef7ec0ea"
     }
    },
    "880dff82e1a74f1199667bf6b553f649": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b46a9cb2df1453aa6cfa60667b1753e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_106bd7e497f449a8b7c9c990589060ce",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_000dcc79395146e398c198ca52093d04",
      "value": 3
     }
    },
    "8bf386f2c2e849a99222da93aa608f09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e5f6259bae04504842f6a02565516cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2fbeb9357db141a1a09f2a7bd18ab50d",
       "IPY_MODEL_e6b099aa61334bbcb4582fff406301bf",
       "IPY_MODEL_cc45d070cce348b09288c0ad73476db0"
      ],
      "layout": "IPY_MODEL_859c5cb0fd084931bd4f28eb29106ec1"
     }
    },
    "8fa8314b1f544b0dbe2a6b66642bb2ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "929e1dc11deb4311af776dcc6912bb64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9899df0bad1e48f8afe0d1b61e07c711": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98ea1352e7f04a4690efd847780499b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f2a6363b53f4ef18e6e3ab24ab2e844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc88e25d2a8b485aa32ecb60ba349e59",
      "placeholder": "​",
      "style": "IPY_MODEL_3ab518d711e540588bcd56e726be8b03",
      "value": " 425087/425733 [00:45&lt;00:00, 10517.28 examples/s]"
     }
    },
    "a0d6d3f3104a49b5a1280877b0acc6fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0fcaced60344b04aa3b6df1095066a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4a598d65d944868b621ba31ebb60774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a65deebb01dd49a3a9dd24088dff5c54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7418b577b4b45819361cbb8c8e8f34f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa53ed3abd16489ca16a07898ca79a27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_453177ecbed04ec69ffe086d3e1abe45",
       "IPY_MODEL_3f9a076e27784393b377bd017bcbea65",
       "IPY_MODEL_6db0d2fb72824c1fb049fae00e37848f"
      ],
      "layout": "IPY_MODEL_f2cce5a1c14d4859b0a8d8bc92532901"
     }
    },
    "ad2c50c10d624241a7189045c98395f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aefbfaafb91d4e61901ea44c28059202": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7677f9e6594badaf4d4d7bbd9e906a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b11727491af741fba5b03ca0817f0061": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be55bb34c88b4dc9bc67129266bb4814": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2fb95ba6d2249108ceae7e3a6702027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7f0434eab6c4f0b9a75f4cecfe75066": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c86f1e18624b49d3b87156b972fcb078": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8b4d5e0b934412cbb1459593437c85b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca603589eb194ffcb70952f53870ee23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc45d070cce348b09288c0ad73476db0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6dcd380a3cf74c20b0bf327b3b2bd4d7",
      "placeholder": "​",
      "style": "IPY_MODEL_c8b4d5e0b934412cbb1459593437c85b",
      "value": " 75507/75720 [00:27&lt;00:00, 3081.35 examples/s]"
     }
    },
    "cf0168a918834141b79519f2b2602d8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_056ec29974324207a825d2f378983269",
      "placeholder": "​",
      "style": "IPY_MODEL_c2fb95ba6d2249108ceae7e3a6702027",
      "value": "Filter: 100%"
     }
    },
    "d00c808399e94ce7bc7681b1924c1317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "d0a9edad721b4bb490cf145d73246752": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "d26aa64f296946a98ca3576896085d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dab207b41b5a49078c819d880d567253": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77575870420841008befbc39cd6e8ca3",
      "placeholder": "​",
      "style": "IPY_MODEL_21bf399e8fe14773bd8534079790d912",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "dac1ae60d627406985222dd877ad20af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9899df0bad1e48f8afe0d1b61e07c711",
      "placeholder": "​",
      "style": "IPY_MODEL_47224a4f20d94fd9a4b7ead5d4d2c8d0",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "db4f34481f3d4432b1131c4454bde02b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0d6d3f3104a49b5a1280877b0acc6fc",
      "placeholder": "​",
      "style": "IPY_MODEL_7dc0ca656f554e239a2fae118cccbff6",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 6.52MB/s]"
     }
    },
    "de7d3989d00048deb875760e44f85cec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e10e4b4aface424b87e8248928a36e32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b11727491af741fba5b03ca0817f0061",
      "placeholder": "​",
      "style": "IPY_MODEL_d26aa64f296946a98ca3576896085d21",
      "value": " 762/762 [00:00&lt;00:00, 40.0kB/s]"
     }
    },
    "e4bb191b4b3644b18a15bf9b73d40025": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3420b2ee5afc4266864da21329ee302c",
      "placeholder": "​",
      "style": "IPY_MODEL_e99ef1590d79471aa075846e13e987df",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 10.1MB/s]"
     }
    },
    "e5e4ac7e29074d219da9230ee9e8383e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a8578d1d78e485aa6eab99cbf4cdc90",
      "max": 401401,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20c6f750d02f475bbf9ca928bac783ed",
      "value": 401401
     }
    },
    "e6b099aa61334bbcb4582fff406301bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39513218f84c424482efec652188b836",
      "max": 75720,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_679e6431ac7f48829e9843309510f6ac",
      "value": 75720
     }
    },
    "e99ef1590d79471aa075846e13e987df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea0f9cbaaa0d4bd9b410aecae4592f70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed05681b63164f6d8e5588f778b08c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea0f9cbaaa0d4bd9b410aecae4592f70",
      "placeholder": "​",
      "style": "IPY_MODEL_055e8d3c5ed1480f8589046c99478264",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "ed68913d91a04c2483cb93096e74e85d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1409216cb4a34c05bff300118052c3cf",
       "IPY_MODEL_8b46a9cb2df1453aa6cfa60667b1753e",
       "IPY_MODEL_15e30cae578648579b18af41a9f108aa"
      ],
      "layout": "IPY_MODEL_aefbfaafb91d4e61901ea44c28059202"
     }
    },
    "ef1083cf5d3f485aaba7cce62b1e74bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_880dff82e1a74f1199667bf6b553f649",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78b4151d77db41218cf85c3a7dd3acf6",
      "value": 1355256
     }
    },
    "f2cce5a1c14d4859b0a8d8bc92532901": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "f4d72d569d114d84a6e84fea58ab9ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f59ec6ee3aeb411598d6aa6ece60723b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b9f7aa5ff474ca7824da8d4deb34879",
      "placeholder": "​",
      "style": "IPY_MODEL_2f382619a1a444638ef8bffe7ab5b79c",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "f75ecc1f5855495b8f0450ccb92ea3d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f78d8af780e3404fadd7e658ec6bdc21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a65deebb01dd49a3a9dd24088dff5c54",
      "placeholder": "​",
      "style": "IPY_MODEL_11fab0bcd0f043a1a9bc9bbf1516c98e",
      "value": " 456k/456k [00:00&lt;00:00, 3.33MB/s]"
     }
    },
    "f96ef97ccb8d44d3b1b0249cd4d146c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9be83a48d9d4ba6a04492be4baa9b68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa3830594d944a0b960a77fa61bd5e83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f5fd6de59394b77b520511f5f82adb9",
      "max": 425733,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_607a074d72954f23b3cc342d8b7729e5",
      "value": 425733
     }
    },
    "fbd757d570cb4829afd6d9de85ada202": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b5f62cbc19e4204878137b4b6efaf9e",
      "max": 401401,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a4a598d65d944868b621ba31ebb60774",
      "value": 401401
     }
    },
    "fc88e25d2a8b485aa32ecb60ba349e59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe8747d657e54583adb6dd4eb48eaad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_70c304c708574f77a272f08efec13b93",
       "IPY_MODEL_1c62c3a9d80e4924b38a496df7b5c474",
       "IPY_MODEL_9f2a6363b53f4ef18e6e3ab24ab2e844"
      ],
      "layout": "IPY_MODEL_8fa8314b1f544b0dbe2a6b66642bb2ea"
     }
    },
    "ff33a5ca496b4222933ed7adf159ff50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
